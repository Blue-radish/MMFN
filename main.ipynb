{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe99407-47a5-431e-851a-b7e1e46ae347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "from qqdm import qqdm, format_str\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import gradio as gr\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from src.model import MultimodalClassifier, MultimodalDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('./model/hfl_rbt6')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8697abdd-3c10-48e5-9946-392544a2184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_web(url):\n",
    "    headers={\n",
    "        \"User-Agent\":\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\",\n",
    "        \"Cookie\":\n",
    "    \"XSRF-TOKEN=deleted; expires=Thu, 01-Jan-1970 00:00:01 GMT; Max-Age=0; path=/; domain=.weibo.cn; HttpOnly\",\n",
    "    \"Referer\":\n",
    "    url\n",
    "    }\n",
    "    \n",
    "    resp=requests.get(url,headers=headers)\n",
    "    \n",
    "    obj=re.compile(r'\"text\":(?P<text>.*?)\"textLength\"',re.S)\n",
    "    obj1=re.compile(r\"[\\u4e00-\\u9fa5,。“”]\",re.S)\n",
    "    obj2=re.compile(r'\"pics\": .*?\"url\": \"(?P<pic_url>.*?)\",',re.S)\n",
    "    results=obj.finditer(resp.text)\n",
    "    chinese_text = \"\"\n",
    "    for result in results:\n",
    "        text=result.group(\"text\")\n",
    "        matches = obj1.finditer(text)\n",
    "        for match in matches:\n",
    "            chinese_text += match.group()\n",
    "    pics=obj2.finditer(resp.text)\n",
    "    pic_url=\"\"\n",
    "    for pic in pics:\n",
    "        pic_url=pic.group(\"pic_url\")\n",
    "    print(pic_url)\n",
    "    pic_name=pic_url.split(\"/\")[-1]\n",
    "    # txt_name=pic_url.split(\"/\")[-1].split(\".\")[0]+\".txt\"\n",
    "    txt_name=\"text.txt\"\n",
    "    \n",
    "    pic_resp=requests.get(pic_url,headers=headers)\n",
    "    \n",
    "    file_path_pic = rf\".\\temp\\imgs\"\n",
    "    file_path_txt = rf\".\\temp\"\n",
    "    if os.path.exists(file_path_txt):\n",
    "        shutil.rmtree(file_path_txt)\n",
    "        os.makedirs(file_path_pic)\n",
    "\n",
    "    with open(os.path.join(file_path_pic, pic_name),\"wb\") as f:\n",
    "        f.write(pic_resp.content)\n",
    "    print(\"pic_over\")\n",
    "    with open(os.path.join(file_path_txt, txt_name),'w', encoding='utf-8') as f:\n",
    "        f.write(chinese_text)\n",
    "    print(\"txt_over\")\n",
    "    resp.close()\n",
    "    pic_resp.close()\n",
    "\n",
    "def web_data(url):\n",
    "    get_web(url)\n",
    "    img_path=r\".\\temp\\imgs\"\n",
    "    for file in os.listdir(img_path):\n",
    "        name = file.split('.')[0]\n",
    "        file_path = os.path.join(img_path, file)\n",
    "        output_path = os.path.join(img_path, f\"{name}.jpg\")\n",
    "        with Image.open(file_path) as i:\n",
    "            img = i.convert('RGB')\n",
    "        os.remove(file_path)\n",
    "        img.save(output_path, 'JPEG')\n",
    "            \n",
    "    with open(r\".\\temp\\text.txt\", 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    arr=[{'text': text, 'img': [], 'label': None}]\n",
    "    for img in os.listdir(img_path):\n",
    "        with open(os.path.join(img_path, img), 'rb') as file:\n",
    "            img_data = file.read()\n",
    "            arr[0]['img'].append(img_data)\n",
    "    with open(r'temp\\test.pkl', 'wb') as file:\n",
    "        pickle.dump(arr, file)\n",
    "        \n",
    "    img = Image.open(output_path)\n",
    "    with open(r\".\\temp\\text.txt\", 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text, img\n",
    "\n",
    "def data(imgs_path, text, model_name):\n",
    "    file_path_pic = rf\".\\temp\\imgs\"\n",
    "    file_path_txt = rf\".\\temp\"\n",
    "    img_path=r\".\\temp\\imgs\"\n",
    "    if os.path.exists(file_path_txt):\n",
    "        shutil.rmtree(file_path_txt)\n",
    "        os.makedirs(file_path_pic)\n",
    "    for file_path in imgs_path:\n",
    "        name = file_path.split('\\\\')[-1].split('.')[0]\n",
    "        output_path = os.path.join(r\".\\temp\\imgs\", f\"{name}.jpg\")\n",
    "        with Image.open(file_path) as i:\n",
    "            img = i.convert('RGB')\n",
    "        img.save(output_path, 'JPEG')\n",
    "    txt_name=\"text.txt\"\n",
    "    with open(os.path.join(file_path_txt, txt_name),'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    with open(r\".\\temp\\text.txt\", 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    arr=[{'text': text, 'img': [], 'label': None}]\n",
    "    for img in os.listdir(img_path):\n",
    "        with open(os.path.join(img_path, img), 'rb') as file:\n",
    "            img_data = file.read()\n",
    "            arr[0]['img'].append(img_data)\n",
    "    with open(r'temp\\test.pkl', 'wb') as file:\n",
    "        pickle.dump(arr, file)\n",
    "    \n",
    "    return FN(model_name)\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     batch_texts = []\n",
    "#     batch_images = []\n",
    "#     for item in batch:\n",
    "#         batch_texts.append(item[0])\n",
    "#         batch_images.append(item[1]) # 将每个样本的所有图片作为一个列表添加\n",
    "#     # 对文字进行预处理\n",
    "#     batch_texts = tokenizer.batch_encode_plus(batch_texts,\n",
    "#                                          truncation=True,\n",
    "#                                          padding=True,\n",
    "#                                          return_tensors='pt',\n",
    "#                                          is_split_into_words=True,\n",
    "#                                          max_length=512-2\n",
    "#                                         )\n",
    "\n",
    "#     # 不在这里转换图像数据为张量，在模型的forward方法中处理\n",
    "#     return batch_texts.to(device), batch_images\n",
    "def collate_fn(batch):\n",
    "    batch_texts = []\n",
    "    batch_images = []\n",
    "    # batch_labels = []\n",
    "\n",
    "    # 遍历批次中的每个样本\n",
    "    for item in batch:\n",
    "        batch_texts.append(item[0])\n",
    "        batch_images.append(item[1].unsqueeze(0))\n",
    "        # batch_labels.append(item[2])\n",
    "\n",
    "    batch_texts = tokenizer.batch_encode_plus(batch_texts,\n",
    "                                              truncation=True,\n",
    "                                              padding=True,\n",
    "                                              return_tensors='pt',\n",
    "                                              is_split_into_words=True,\n",
    "                                              max_length=512-2\n",
    "                                             )\n",
    "    batch_images = torch.stack(batch_images).squeeze(1)\n",
    "\n",
    "    # batch_labels = torch.tensor(batch_labels)\n",
    "\n",
    "    # return batch_texts.to(device), batch_images.to(device), batch_labels.to(device)\n",
    "    return batch_texts.to(device), batch_images.to(device)\n",
    "\n",
    "def FN(model_name):\n",
    "    print(model_name)\n",
    "    test_path = './temp/test.pkl'\n",
    "    model_path = f'./model/{model_name}'\n",
    "    with open(test_path, 'rb') as file:\n",
    "        test_data = pickle.load(file)\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "    test_dataset = MultimodalDataset(test_path)\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                    batch_size=1,\n",
    "                    collate_fn=collate_fn,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True)  # 如果最后一个批次不足batch_size，则丢弃\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    # 不计算梯度\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch_texts, batch_images = batch\n",
    "            outputs = model(batch_texts['input_ids'], batch_texts['attention_mask'], batch_images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # print(f\"假新闻: {outputs[0][0].item()*100:.2f}%\\t真新闻: {outputs[0][1].item()*100:.2f}%\")\n",
    "            return f\"假新闻概率: {outputs[0][0].item()*100:.4f}%\\t\\t真新闻概率: {outputs[0][1].item()*100:.4f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd127a4c-7aff-4582-b08d-fcf12b16663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name='FN', num_epoch=3, bert=\"hfl_rbt6\", lr=1e-5, batch_size=8, train_data='', test_data=''):\n",
    "    def collate_fn(batch):\n",
    "        batch_texts = []\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        for item in batch:\n",
    "            batch_texts.append(item[0])\n",
    "            batch_images.append(item[1]) # 将每个样本的所有图片作为一个列表添加\n",
    "            batch_labels.append(item[2])\n",
    "        # 对文字进行预处理\n",
    "        batch_texts = tokenizer.batch_encode_plus(batch_texts,\n",
    "                                             truncation=True,\n",
    "                                             padding=True,\n",
    "                                             return_tensors='pt',\n",
    "                                             is_split_into_words=True,\n",
    "                                             max_length=512-2\n",
    "                                            )\n",
    "        # 标签\n",
    "        batch_labels = torch.tensor(batch_labels)\n",
    "        # 不在这里转换图像数据为张量，在模型的forward方法中处理\n",
    "        return batch_texts.to(device), batch_images, batch_labels.to(device)\n",
    "    \n",
    "    lr = float(lr)\n",
    "    dataset = MultimodalDataset(train_data)\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    collate_fn=collate_fn,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True)\n",
    "    best_loss = 99\n",
    "    model_path = \"./model\" # 模型存放路径\n",
    "    model = MultimodalClassifier(f'./model/{bert}', './model/img_model/resnet50.pth').to(device)\n",
    "    criterion = nn.CrossEntropyLoss() # 交叉熵损失\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    progress=gr.Progress()\n",
    "    for epoch in progress.tqdm(range(1, num_epoch+1), desc='训练中...'):\n",
    "        tot_loss = []\n",
    "        for batch in loader:\n",
    "            batch_texts, batch_images, batch_labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_texts['input_ids'], batch_texts['attention_mask'], batch_images)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            tot_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        mean_loss = np.mean(tot_loss)\n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            torch.save(model, os.path.join(model_path, f'{name}_best.pth'))\n",
    "        torch.save(model, os.path.join(model_path, f'{name}_last.pth'))\n",
    "        \n",
    "    if test_data != '':\n",
    "        model = torch.load(os.path.join(model_path, f'{name}_best.pth'), map_location=device)\n",
    "        test_dataset = MultimodalDataset(test_data)\n",
    "        test_loader = DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=16,\n",
    "                                 collate_fn=collate_fn,\n",
    "                                 shuffle=True,\n",
    "                                 drop_last=True)  # 如果最后一个批次不足batch_size，则丢弃\n",
    "        y_true = []\n",
    "        y_scores = []\n",
    "        y_pred = []\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            progress = gr.Progress()\n",
    "            for batch in progress.tqdm(test_loader, desc='测试中...'):\n",
    "                batch_texts, batch_images, batch_labels = batch\n",
    "                outputs = model(batch_texts['input_ids'], batch_texts['attention_mask'], batch_images)\n",
    "                y_true.extend(batch_labels.tolist())\n",
    "                y_scores.extend(outputs[:, 1].tolist())  # 正类标签为1\n",
    "                y_pred.extend((outputs[:, 1] > 0.5).int().tolist())  # 阈值为0.5\n",
    "    \n",
    "        # 计算正确率\n",
    "        correct_predictions = sum([1 for true, pred in zip(y_true, y_pred) if true == pred])\n",
    "        accuracy = correct_predictions / len(y_true)\n",
    "    \n",
    "        fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.rcParams['font.sans-serif']=['SimHei']  # 用来正常显示中文标签 \n",
    "        plt.rcParams['axes.unicode_minus']=False  # 用来正常显示负号\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=lw, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('误报率')\n",
    "        plt.ylabel('真正例率')\n",
    "        plt.title('ROC曲线')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        return f\"best_loss={best_loss} \\t\\t last_loss={mean_loss}\", plt.gcf(), accuracy\n",
    "    else:\n",
    "        return f\"best_loss={best_loss} \\t\\t last_loss={mean_loss}\", None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599c3cdc-b0b7-49d7-8abf-2c9de8a4657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://wx1.sinaimg.cn/orj360/006GcCQIly1hqtrhg9lxrj30j60kggnf.jpg\n",
      "pic_over\n",
      "txt_over\n",
      "https://wx1.sinaimg.cn/orj360/6c709881ly1hsj8z7ydbtj20u01hcwy3.jpg\n",
      "pic_over\n",
      "txt_over\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as web:\n",
    "    with gr.Tab(label=\"inference\"):\n",
    "        with gr.Row(): # 水平\n",
    "            with gr.Column(scale=3): # 垂直\n",
    "                model_list = [f for f in os.listdir(\"./model\") if f.endswith('.pth')]\n",
    "                inf_model = gr.Dropdown(choices=model_list, label=\"选择模型\")\n",
    "            with gr.Column(scale=1):\n",
    "                inf_run = gr.Button(variant=\"primary\")\n",
    "        with gr.Row(): # 水平\n",
    "            with gr.Column(scale=2):\n",
    "                inf_imgs = gr.File(file_count=\"multiple\", type=\"filepath\", label=\"Upload Images\")\n",
    "            with gr.Column(scale=3):\n",
    "                inf_text = gr.Textbox(label=\"文字\")\n",
    "        with gr.Row(): # 水平\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Image(value=\"./model/img_model/bert.jpg\")\n",
    "            with gr.Column(scale=3):\n",
    "                inf_output = gr.Textbox(label=\"output\")\n",
    "        inf_run.click(fn=data, inputs=[inf_imgs, inf_text, inf_model], outputs=inf_output)\n",
    "    \n",
    "    with gr.Tab(label=\"web_inference\"):\n",
    "        with gr.Row(): # 水平\n",
    "            with gr.Column(scale=5): # 垂直\n",
    "                web_http = gr.Textbox(label=\"input\")\n",
    "            with gr.Column(scale=3): # 垂直\n",
    "                model_list = [f for f in os.listdir(\"./model\") if f.endswith('.pth')]\n",
    "                web_model = gr.Dropdown(choices=model_list, label=\"选择模型\")\n",
    "            with gr.Column(scale=1):\n",
    "                web_get = gr.Button(variant=\"primary\", value=\"Get\")\n",
    "                web_run = gr.Button(variant=\"primary\")\n",
    "        web_output = gr.Textbox(label=\"output\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                web_img = gr.Image()\n",
    "            with gr.Column(scale=3):\n",
    "                web_text = gr.Textbox(label=\"文字\")\n",
    "        web_run.click(fn=FN, inputs=web_model, outputs=web_output)\n",
    "        web_get.click(fn=web_data, inputs=web_http, outputs=[web_text, web_img])\n",
    "        \n",
    "    with gr.Tab(label=\"train\"):\n",
    "        with gr.Row(): # 水平\n",
    "            with gr.Column(scale=2):\n",
    "                tr_name = gr.Textbox(label=\"实验名称\")\n",
    "            with gr.Column(scale=2):\n",
    "                tr_epoch = gr.Slider(\n",
    "                    minimum=1,\n",
    "                    maximum=100,\n",
    "                    value=3,\n",
    "                    step=1,\n",
    "                    label=\"epoch\",\n",
    "                    interactive=True,\n",
    "                )\n",
    "            with gr.Row(): # 水平\n",
    "                with gr.Column(scale=3):\n",
    "                    tr_run = gr.Button(variant=\"primary\", value=\"训练\")\n",
    "                with gr.Column(scale=3):\n",
    "                    tr_bert = gr.Radio(choices=[\"hfl_rbt6\", \"bert-chinese\"], label=\"bert底模\", value=\"hfl_rbt6\")\n",
    "        with gr.Row(): # 水平\n",
    "            with gr.Column(scale=3):\n",
    "                tr_lr = gr.Textbox(label=\"学习率\", value=0.00001)\n",
    "            with gr.Column(scale=3):\n",
    "                tr_batch_size = gr.Slider(\n",
    "                    minimum=1,\n",
    "                    maximum=256,\n",
    "                    value=8,\n",
    "                    step=1,\n",
    "                    label=\"batch_size\",\n",
    "                    interactive=True,\n",
    "                )\n",
    "        with gr.Row(): # 水平\n",
    "            with gr.Column():\n",
    "                train_data = gr.Textbox(label=\"训练集路径\")\n",
    "            with gr.Column():\n",
    "                test_data = gr.Textbox(label=\"测试集路径\")\n",
    "        with gr.Row(): # 水平\n",
    "            tr_info = gr.Textbox(label=\"输出信息\")\n",
    "        with gr.Row(): # 水平\n",
    "            with gr.Column():\n",
    "                plot_output = gr.Plot(label=\"ROC Curve\")\n",
    "            with gr.Column():\n",
    "                test_output = gr.Textbox(label=\"正确率\")\n",
    "        tr_run.click(fn=training, inputs=[tr_name, tr_epoch, tr_bert, tr_lr, tr_batch_size, train_data, test_data], \n",
    "                     outputs=[tr_info, plot_output, test_output])\n",
    "\n",
    "os.environ[\"no_proxy\"] = \"localhost,127.0.0.1,::1\"\n",
    "web.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e6029-d424-4e20-b26c-f42ca1dbfa28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
